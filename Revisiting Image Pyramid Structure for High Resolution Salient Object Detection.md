## <center> Revisiting Image Pyramid Structure for High Resolution Salient Object Detection
### 摘要
提出了一种基于图像金字塔的SOD框架，反向显著性金字塔重建网络，用于无需任何HR数据集的HR预测，从统一图像的一对LR和HR尺度合成两个不同的图像金字塔，以克服有效的感受野差异。为解决不同尺度图像感受野不同的问题，文章提出了两种相互关联的解决方案。

首先设计了一个能够合并多个结果的网络结构，而不管输入的大小，也就是InSPyReNet，图像金字塔是一种简单而直接的图像混合方法，因此用InSPyReNet来直接生成显著性图的图像金字塔。其次，设计了一种金字塔混用技术，用于不同尺度下重叠显著性图的两个图像金字塔，

### Model Architecture
使用ResNet和Swin Transformer作为骨干网络，但对于HR预测，我们只使用Swin作为骨干，根据UACANet我们将并行轴向注意力编码器用于多尺度编码器，以减少骨干特征图的通道数，并将并行轴向注意解码器用于预测最小阶段上的初始显著图，两个模块通过非本地操作捕获全局上下文，并且由于轴向注意机制，效率很高。
<div align="center"><img src="image\InSPyReNet.PNG"></img></div>
本文从Stage3开始进行图像重建，为了恢复Stage1和Stage0的规模，作者在适当位置使用双线性插值。我们在每个阶段定位一个基于自我注意的解码器，也就是尺度不变的上下文注意(SICA)，以预测显著性图的拉普拉斯图像，根据预测的拉普拉斯显著性图，我们可以从较高阶段到较低阶段重构显著性图。

基于注意力的像素级预测解码器由于其相对空间维度的非局部操作而表现出良好的性能，然而，当输入图像的大小大于训练设置时，通常无法产生适当的结果，原因如下：（1）当输入图像的尺寸足够大时，对于根据空间维度展开特征图并进行矩阵乘法的非局部操作存在训练推理差异，例如，从非局部运算取决于输入图像的空间维度，此外，非局部运算的复杂度随着输入大小的增加呈二次方增加。因此提出了SICA，一种用于鲁棒拉普拉斯显著性预测的尺度不变上下文注意模块。SICA的具体结构如下：
<div align="center"><img src="image\SICA.PNG"></img></div>
由于计算对象区域表示会导致训练推理差异，因此作者根据训练时间的形状调整输入特征图x和上下文图c的大小，因为在训练步骤中，图像已经被重新塑造成固定形状，因此不必调整它们的大小，对于上下文映射，与OCRNet不同，我们只能访问不足的显著性映射，但是这还是不够，因此又设计生成了以下几个上下文映射，背景图选择和方程的进一步细节可以在补充材料中介绍。使用SICA，可以更精确地计算HR图像中的拉普拉斯显著性图，因此可以将金字塔混合应用于HR预测。

加普拉斯金字塔是一种图像压缩技术，针对每个尺度存储低通滤波图像和原始图像之间的差异，也就是作为的高频细节，加入这种技术来设计网络结构，以专注于边界细节，并将显著性图从最小阶段重建到原始大小，作者从最上层的显著性图开始初始显著性图，并从拉普拉斯显著性图像聚集高频细节。将第j阶段的显著性图和拉普拉斯显著图分别表示为$S^j$和$U^j$，为了重建从第j+1到j阶段的显著性图，使用EXPAND运算，也就是：
$$\Large S_e^j(x,y)=4\sum_{m=-3}^3\sum_{n=-3}^3g(m,n)S^{j+1}(\frac{x-m}{2},\frac{y-n}{2})$$
其中$g(m,n)$表示高斯滤波器，其中核大小和标准差根据经验设置为7和1.为了恢复显著性细节，作者将SICA中的拉普拉斯显著性图添加如下：$\Large S^j=S^j_e+U^j$重复这个过程直到获得最低阶段，并将其作用在最终预测上。

监督多级网侧输出的网络的一种典型方法就是对每个级的预测使用双线性插值并计算和GT的损失函数。但是当从较高阶段预测的显著性图在空间维度上较小，这可能导致阶段尺度不一致，特别是对于显著对象的边界区域。由于第三阶段的显著性输出在物理上无法超过第二阶段的细节，因此作者选择为每个阶段提供一个合适的基础真相，为此创建了GT的图像金字塔。首先从$G_j$阶段获得的基础GT$G_{j-1}$,对应的操作为REDUCE
$$\Large G^j(x,y)=\sum_{m=-3}^3\sum_{n=-3}^3g(m,n)G^{j-1}(2x+m,2y+n)$$
对于损失函数，我们使用二进制交叉熵损失和像素位置感知加权策略，为了鼓励生成的拉普拉斯显著性图遵循金字塔结构，我们结构了$S^{j-1}$到$S^j$级，通过REDUCE操作获得$S_j$,然后使用金字塔一致性损失$L^{PC}$来增强$S_j$和简化显著性图之间的相似性。
$$\Large L^{PC}(S^j,\widetilde{S}^j)=\sum_{(x,y)\in I^j}||S^j(x,y)-\widetilde{S}^j(x,y)||_1$$
$L^{PC}$
通过训练过程正则化较低阶段的显著性图以跟踪图像金字塔结构，总损失的定义为：

$$\Large L(S,G)=\sum_{j=0}^3 \lambda_jL^{wbce}(S^j,G^j)+\eta\sum_{j=0}^2\lambda_jL^{PC}(S^j,\widetilde{S}^j) $$
设置$\eta=10^{-4}$和$\lambda_j=4^j$来平衡各个阶段的损失

最后，我们为SICA的显著性图输入和来自更高阶段的重构过程添加了停止梯度，以迫使每个阶段的显著性输出在训练时间内聚焦于每个尺度，并且仅在推理时间内相互影响。该策略通过明确地防止来自较低阶段的梯度流影响较高阶段来鼓励逐阶段GT方案。因此，具有高频细节的监控不会影响高级解码器，高级解码器仅具有突出对象的抽象形状。虽然此策略可能会影响多尺度方案的性能，但我们使用来自多尺度编码器和SICA不同阶段的特征图来补偿此问题。 
为解决尺度放大所带来的ERF的影响，本文的显著性金字塔输出的直接应用是从不同输入组装多个显著性金字塔，首先使用InSPyReNet为原始图像和调整大小的图像生成显著性金字塔，如下图所示：
<div align="center"><img src="image\InSPyReNet可视化.PNG"></img></div>也就是LR和HR显著性金字塔，然后从LR金字塔的最低阶段开始，而是不是从HR金字塔重建显著性图，更直接的说，LR金字塔是用HR金字塔扩展的，因此构建了一个7级显著性金字塔。
对于HR金字塔重建，计算前一阶段显著性图的扩张和腐蚀操作，并将其减去，已获得拉普拉斯显著图中的过渡区域并与之相乘，过渡区域用于过滤HR金字塔中不需要的噪声，因为我们需要应用的边界细节应该仅存在于边界区域周围。