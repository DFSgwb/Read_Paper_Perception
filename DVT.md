<div align="center"><img src=".\image\DVT.PNG"></div>
如上图所示，对于每个测试样本，首先使用少量的1D标记嵌入粗表示，这个可以通过直接展平分割图像补丁或利用令牌到令牌模块等技术来实现，用个几个标记推断出一个VIT已获得快速预测，由于Transformers的计算成本相对于令牌呈二次时间复杂度。因此该过程具有较高的效率，然后使用一定的标准对预测进行评估，以确定其是否足够可靠，可以立即检索，当对于模型的可靠性的置信度较高时，就可以提前终止。一旦预测不符合终止准则，原始输入图像将被分割成更多的标记，以进行更精确但计算量更大的推断，这里的每个标记嵌入的维度保持不变，而标记的数量增加，从而支持更细粒度的表示，将激活一个与前一个具有相同体系结构但参数不同的附加transformer，通过设计，这一阶段在一些困难测试样本上权衡计算以获得更高的精确度，为了提高效率，新模型可以重用先前学习的特征和关系。同样，在获得新的预测后，将应用终止标准，并继续执行上述程序，直到样本退出或推断出最终的transformer。
对于训练而言：优化的目标就是：

$$\large minimize=\frac{1}{|D_{train}|}\sum(x,y)\in D_{min}[\sum_iL_{CE}(p_i,y)]$$
其中$(x,y)$表示训练集$D_{train}$中的样本及其相应标签，采用交叉熵损失函数，而$p_i$表示第i出口输出的softmax预测概率。

本文提出两种机制来重用所学的深层特征和自我注意力关系，这两种方式都可以通过最小的额外计算成本来显著提高测试精度。