<div align="center"><img src=".\image\DVT.PNG"></div>
如上图所示，对于每个测试样本，首先使用少量的1D标记嵌入粗表示，这个可以通过直接展平分割图像补丁或利用令牌到令牌模块等技术来实现，用个几个标记推断出一个VIT已获得快速预测，由于Transformers的计算成本相对于令牌呈二次时间复杂度。因此该过程具有较高的效率，然后使用一定的标准对预测进行评估，以确定其是否足够可靠，可以立即检索，当对于模型的可靠性的置信度较高时，就可以提前终止。一旦预测不符合终止准则，原始输入图像将被分割成更多的标记，以进行更精确但计算量更大的推断，这里的每个标记嵌入的维度保持不变，而标记的数量增加，从而支持更细粒度的表示，将激活一个与前一个具有相同体系结构但参数不同的附加transformer，通过设计，这一阶段在一些困难测试样本上权衡计算以获得更高的精确度，为了提高效率，新模型可以重用先前学习的特征和关系。同样，在获得新的预测后，将应用终止标准，并继续执行上述程序，直到样本退出或推断出最终的transformer。
对于训练而言：优化的目标就是：

$$\large minimize=\frac{1}{|D_{train}|}\sum(x,y)\in D_{min}[\sum_iL_{CE}(p_i,y)]$$
其中$(x,y)$表示训练集$D_{train}$中的样本及其相应标签，采用交叉熵损失函数，而$p_i$表示第i出口输出的softmax预测概率。

本文提出两种机制来重用所学的深层特征和自我注意力关系，这两种方式都可以通过最小的额外计算成本来显著提高测试精度。

DVT中的所有transformer都有相同的目标，也就是提取有区别的表示以进行准确识别，因此，下游模型应该在先前获得的深层特征的基础上学习，而不是从头开始提取特征，前者效率更高，因为上游模型中执行的计算对自身和后续模型都有贡献，为了实现这个想法，提出了一种特征重用机制，利用上游Transformer的最后一层输出的图像标记，也就是$Z^{IP}_{L}$来学习下游模型的