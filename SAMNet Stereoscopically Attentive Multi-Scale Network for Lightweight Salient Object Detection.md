# <center>SAMNet: Stereoscopically Attentive Multi-Scale Network for Lightweight Salient Object Detection </center>
### Abstract

为减轻网络规模和计算开销,便于在移动设备上的部署,设计了一种立体注意多尺度(SAM)模块,该模块采用立体注意机制来自适应地融合各种尺度的特征。并随之提出了一种非常轻量级的网络(SAMNet)。

轻量化SOD模型的难点:

     1.SOD需要高级抽象语义特征和低级细粒度特征来分别定位显著对象和细化对象细节
     2.SOD需要多尺度信息来处理自然场景中具有各种大小和纵横比的显著对象，
由于轻量级网络通常具有较浅的深度和简化的操作,因此和传统的大型网络相比,它们在多层次和多尺度学习方面的效率较低。

### 多尺度学习
采用具有不同扩张率的扩张卷积来捕获多尺度信息，并使用深度可分离卷积来减少浮点运算和模型参数。我们将其称为扩张的深度可分离卷积，并将其用作基本的卷积算子，以在深度和宽度方面扩大网络维度。
### <center>SAMNet网络结构图</center>
<div align="center"><img src="image\SAMNet网络结构.PNG"></div>


其中$I\in \mathbb{R}^{C\times H\times W}$表示输入特征图。对于每个输入的$I$，首先使用单个深度可分离的$DsConv3\times 3$来提取每个分支的公共信息$F_0$,在不同分支使用不同扩张率的$DsConv3\times 3$然后不同尺度的上下文信息通过带有残差连接的单个元素求和进行聚合。也就是
$$\Large F=\sum_{i=0}^NF_i$$
其中$F_i$表示不用分支得到的特征信息，使用$element-wise-summation$ 而不是级联的原因是级联的方式将大大增加通道的数量，导致更高的计算量。然后通过vanilla$DsConv1\times1$将聚合特征重新进行排列，也就是$O =\mathcal{K}_{fuse}(F)+I$其中$\mathcal{K}$表示$Conv1\times1$操作，用于在不同尺度上融合上下文信息。当输入特征$I$是高分辨时，需要更大的扩张率和更多的分支。因为大型特征映射通常具有不同尺度的上下文信息。但其中的不足来自于按元素求和，这样做虽然可以降低计算复杂度，但是也会导致当来自不同分支的上下文信息直接汇总在一起时，信息分支可能会被非信息分支削弱甚至是压制。另一方面，来自不同深度的不同尺度信息本应该在其中占据不同重要程度，但是按元素求和会导致每个尺度的信息的重要程度都相同。因此提出一种立体注意力机制来使得每个空间位置的每个通道通过软注意力机制自适应地调整每个分支的权重。

### 立体注意机制

我们认为输入特征是一个四维张量$F\in\mathbb{R}^{(N+1)\times C\times H\times W}$其中每个分支$i\in \{0,...,N\}$生成不同尺度和语义水平的$F_i\in\mathbb{R}^{C\times H\times W}$,一般来说,网络中不同深度的层可能更喜欢来自不同尺度水平的信息，因此盲目的输入所有的特征$F可能导致严重的过拟合，因此本文使用注意力机制来实现专注地抑制非信息分支，并主动促进有区别的分支。注意力模块应该具有的特征:

(1)由于每个通道的独立性,最终的注意力应该具有很强的通道内依赖性,更具体地说，通常通过将独立滤波器与输入特征进行卷积来获得不同的特征通道。在这种情况下，如果来自特定通道的特征对最终预测具有信息性,则同一分支的同一通道中的特征也有可能具有信息性。

(2)最终的注意力应该具有很强的空间依赖性，SOD需要对每个像素的局部邻域进行一定程度的推理。

(3)计算的高效性

我们将注意力权重分解为两个权重，也就是
$$V=d\otimes s$$

其中$d$和$s$分别表示通道注意力和空间注意力。$\otimes$是指按元素乘法。具体的来说$channel-wise-attention$是在$d\in \mathbb{R}^{(N+1)\times C}$上进行空间收缩特征，并注意抑制非信息性特征，并以通道全局方式促进判别性特征。同样，空间注意力$s∈\mathbb{R}^{(N+1)×H×W}$通过不同通道吸收特定空间位置的特征。 最后将d和s广播到$(N+1)×C×H×W$的相同维度上，通过方程得到最终的注意力权重。通道注意力机制旨在计算通道注意向量$W_{i}^D\in \mathbb{R}^C$，对于每个分支i,$i=0,1,2...N$，为了获取不同通道之间的关系，我们使用全局平均池化将全局信息嵌入到融合特征映射上。即：
$$\Large z_c = f_{GAP}(M)=\frac{1}{HW}\sum_{i=1}^H\sum_{j=1}^W M_{c,i,j} (c=0,1,...C-1)$$
然后我们在潜在向量上应用具有两层MLP。并在不同尺度下提取通道信息。

空间注意力旨在计算空间注意图$W_i^S\in\mathbb{R}^{H\times W}$以突出或抑制特定位置的激活关系，总所周知，大的感受野可以更好的捕获语境信息，因此我们选择使用采用扩展卷积的方式来扩大感受野，同时保持较低的计算复杂度。具体过程为融合特征M首先通过$Conv 1\times 1$投影到低维空间$\mathbb{R}^{C/4\times H \times W}$以减少参数量和计算成本。然后将两个扩展的$DsConv3\times 3$用于简化特征，以实现有效的上下文信息整合。最后使用$Conv1\times1$将特征再次缩减为$\mathbb{R}^{(N+1)\times H \times W}$

### 网络架构
### <center>编解码网络结构</center>
<div align="center"><img src="image\SAMNet编解码网络结构.PNG"></div>